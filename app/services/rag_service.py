from app.services.llm_provider_service import llm_service
from app.models.user import User as UserModel
from app.llm_tools.tool_schemas import AVAILABLE_TOOLS_SCHEMAS
from app.llm_tools.tool_functions import TOOL_FUNCTIONS
from typing import Dict, List, Optional, Any, AsyncGenerator, Set, Tuple
from app.core.config import settings
import json
from datetime import datetime
import inspect
import traceback
from cachetools import TTLCache
import asyncio
import re

MAX_TOOL_ITERATIONS = 5

class RAGService:
    def __init__(self):
        """Initializes the RAGService, setting up a cache for tool executions."""
        self.tool_execution_cache = TTLCache(maxsize=200, ttl=60)

    def _summarize_user_profile(self, user: UserModel) -> str:
        """Creates a concise text summary of a user's profile.

        Args:
            user (UserModel): The user object from the database.

        Returns:
            str: A string summarizing the user's preferences and background.
        """
        summary = f"User: {user.username} (Email: {user.email})\n"
        if user.trading_experience:
            summary += f"Trading Experience: {user.trading_experience.value}\n"
        if user.risk_appetite:
            summary += f"Risk Appetite: {user.risk_appetite.value}\n"
        if user.investment_goals:
            summary += f"Investment Goals: {user.investment_goals.value}\n"
        if user.preferred_asset_classes:
            summary += f"Preferred Assets: {', '.join(user.preferred_asset_classes)}\n"
        if user.interests_for_feed:
            summary += f"Interests: {', '.join(user.interests_for_feed)}\n"
        return summary.strip()

    def _clean_llm_json_response(self, llm_response_str: str) -> str:
        """Extracts a clean JSON string from a raw LLM response.

        LLMs often wrap JSON in markdown, comments, or other text. This function
        tries various methods to isolate a valid JSON object or array string.

        Args:
            llm_response_str (str): The raw string output from the LLM.

        Returns:
            str: A cleaned string that is likely a JSON object or array, or the
                 original string if no JSON is found.
        """
        cleaned_str = re.sub(r"<think>.*?</think>", "", llm_response_str, flags=re.DOTALL).strip()
        
        json_match_explicit = re.search(r"```json\s*(.*?)\s*```", cleaned_str, re.DOTALL | re.IGNORECASE)
        if json_match_explicit:
            return json_match_explicit.group(1).strip()

        json_match_generic = re.search(r"```\s*(.*?)\s*```", cleaned_str, re.DOTALL)
        if json_match_generic:
            return json_match_generic.group(1).strip()

        first_brace = cleaned_str.find('{')
        last_brace = cleaned_str.rfind('}')
        if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
            potential_json_object = cleaned_str[first_brace : last_brace + 1]
            try:
                json.loads(potential_json_object)
                return potential_json_object
            except json.JSONDecodeError:
                pass

        first_bracket = cleaned_str.find('[')
        last_bracket = cleaned_str.rfind(']')
        if first_bracket != -1 and last_bracket != -1 and first_bracket < last_bracket:
            potential_json_array = cleaned_str[first_bracket : last_bracket + 1]
            try:
                json.loads(potential_json_array)
                return potential_json_array
            except json.JSONDecodeError:
                pass
                
        return cleaned_str
        
    async def _execute_tool(self, tool_name: str, tool_args_from_llm: Dict[str, Any], user_id: Optional[int] = None) -> Tuple[str, Dict[str, Any]]:
        """Executes a tool function with validation, caching, and error handling.

        This method finds the requested tool, validates the arguments provided by
        the LLM against the function's signature, attempts to coerce types,
        checks for a cached result, and then executes the tool.

        Args:
            tool_name (str): The name of the tool to execute.
            tool_args_from_llm (Dict[str, Any]): The arguments for the tool,
                                                 as generated by the LLM.
            user_id (Optional[int]): The ID of the current user, to be passed
                                     to tools that require it.

        Returns:
            Tuple[str, Dict[str, Any]]: A tuple containing the stringified
                                        result of the tool execution and the
                                        final arguments that were actually used.
        """
        if tool_name not in TOOL_FUNCTIONS:
            error_msg = f"Error: Tool '{tool_name}' not found."
            print(error_msg)
            return error_msg, tool_args_from_llm

        tool_function = TOOL_FUNCTIONS[tool_name]
        sig = inspect.signature(tool_function)
        
        final_tool_args = {}
        missing_required_args = []

        if 'user_id' in sig.parameters and user_id is not None:
            final_tool_args['user_id'] = user_id

        for param_name, param_obj in sig.parameters.items():
            if param_name == 'user_id':
                continue

            if param_name in tool_args_from_llm:
                value = tool_args_from_llm[param_name]
                param_type = param_obj.annotation
                coerced_value = value
                try:
                    if param_type is str and isinstance(value, str):
                        coerced_value = value.strip('"\'')
                    elif param_type is int:
                        if isinstance(value, str): coerced_value = int(value.strip())
                        elif isinstance(value, (int, float)): coerced_value = int(value)
                        else: raise ValueError(f"Cannot convert type {type(value)} to int")
                    elif param_type is float:
                        if isinstance(value, str): coerced_value = float(value.strip())
                        elif isinstance(value, (int, float)): coerced_value = float(value)
                        else: raise ValueError(f"Cannot convert type {type(value)} to float")
                except (ValueError, TypeError) as e:
                    print(f"Warning: Could not coerce/sanitize arg '{param_name}' value '{value}' for tool {tool_name}: {e}. Skipping.")
                    continue
                final_tool_args[param_name] = coerced_value
            elif param_obj.default is inspect.Parameter.empty and param_name not in final_tool_args:
                missing_required_args.append(param_name)

        if missing_required_args:
            error_msg = f"Error: Missing required argument(s) for tool '{tool_name}': {', '.join(missing_required_args)}."
            print(error_msg)
            return error_msg, final_tool_args

        for arg_name in tool_args_from_llm:
            if arg_name not in sig.parameters:
                print(f"Warning: Argument '{arg_name}' (value: {tool_args_from_llm[arg_name]}) not accepted by tool {tool_name}. Ignoring.")

        cache_key_args_list = []
        for k, v in sorted(final_tool_args.items()):
            if isinstance(v, (dict, list)):
                cache_key_args_list.append((k, json.dumps(v, sort_keys=True)))
            else:
                cache_key_args_list.append((k,v))
        cache_key = (tool_name, tuple(cache_key_args_list))


        if cache_key in self.tool_execution_cache:
            print(f"Cache HIT for tool: {tool_name} with args: {final_tool_args}")
            cached_result_str = self.tool_execution_cache[cache_key]
            return cached_result_str, final_tool_args
        print(f"Cache MISS for tool: {tool_name} with args: {final_tool_args}")

        try:
            print(f"Executing tool: {tool_name} with final_tool_args: {final_tool_args}")
            if inspect.iscoroutinefunction(tool_function):
                result = await tool_function(**final_tool_args)
            else:
                result = tool_function(**final_tool_args)
            
            result_str = json.dumps(result) if isinstance(result, (dict, list)) else str(result)
            print(f"Tool {tool_name} result (first 300 chars): {result_str[:300]}...")

            self.tool_execution_cache[cache_key] = result_str

            return result_str, final_tool_args
            
        except Exception as e:
            print(f"Error executing tool {tool_name} with args {final_tool_args}: {e}")
            traceback.print_exc()
            error_message = f"Error during {tool_name} execution: {str(e)}. Arguments used: {json.dumps(final_tool_args)}."
            
            if tool_name in ["get_stock_price", "get_company_overview", "get_historical_stock_data", "get_intraday_stock_data", "get_income_statement", "get_balance_sheet", "get_cash_flow_statement", "get_company_earnings"]:
                symbol = final_tool_args.get('symbol')
                if symbol:
                    error_message += f" Please double-check if '{symbol}' is a valid stock ticker symbol on exchanges covered by Yahoo Finance."
                    
            elif tool_name in ["get_crypto_price", "get_daily_crypto_data"]:
                symbol = final_tool_args.get('symbol')
                market = final_tool_args.get('market')
                if symbol:
                    error_message += f" Please double-check if '{symbol}' is a valid cryptocurrency symbol, potentially paired with '{market or settings.ALPHA_VANTAGE_CRYPTO_MARKET_DEFAULT}'."

            elif tool_name in ["get_asset_price_change_24h"]:
                symbol = final_tool_args.get('symbol')
                if symbol:
                    error_message += f" Please double-check if '{symbol}' is a valid symbol for a stock, crypto (e.g., BTC-USD), or FX pair (e.g., EURUSD=X)."
            
            elif tool_name in ["get_portfolio_positions", "get_portfolio_market_value", "get_portfolio_daily_change_percentage", "get_portfolio_pnl"]:
                portfolio_name = final_tool_args.get('portfolio_name')
                portfolio_id = final_tool_args.get('portfolio_id')
                user_id_arg = final_tool_args.get('user_id')
                if user_id_arg is not None:
                    if not portfolio_name and not portfolio_id:
                        error_message += f" User {user_id_arg} has portfolios, but the request did not specify which one (name or ID). Use `list_my_portfolios` first if needed."
                    else:
                        error_message += f" Ensure the specified portfolio ({portfolio_name or portfolio_id}) exists and belongs to user {user_id_arg}."
            return error_message, final_tool_args

    async def generate_intelligent_response(
        self,
        user_query: str,
        current_user: UserModel, 
        chat_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[str, None]:
        """Generates a response by reasoning and using tools in a loop.

        This function implements a ReAct (Reason-Act) agent. It iteratively
        prompts the LLM to decide on an action (use a tool or respond directly).
        If a tool is chosen, it's executed, and the result is fed back into the
        loop. This continues until the query is answered or a limit is reached,
        at which point a final synthesis step generates the user-facing answer.

        Args:
            user_query (str): The user's latest message.
            current_user (UserModel): The currently authenticated user.
            chat_history (Optional[List[Dict[str, str]]]): The previous messages
                                                           in the conversation.

        Yields:
            str: Chunks of the final generated response.
        """
        normalized_query = user_query.lower().strip()
        simple_greetings = ["hello", "hi", "hey", "good morning", "good afternoon", "good evening", "hiya", "howdy"]
        simple_closings_thanks = ["thanks", "thank you", "thx", "ty", "appreciate it", "ok thanks"]
        simple_closings_bye = ["bye", "goodbye", "see ya", "later", "cya"]
        simple_banter = ["how are you", "how are you doing", "what's up", "sup", "hows it going"]

        if normalized_query in simple_greetings:
            simple_response_messages = [{"role": "system", "content": "You are a friendly and concise assistant. Respond warmly to the user's greeting."}, {"role": "user", "content": user_query}]
            async for chunk in llm_service.generate_streamed_response(messages=simple_response_messages, use_smaller_model=True): yield chunk
            yield "\n"; return
        if any(phrase == normalized_query for phrase in simple_closings_thanks) or any(normalized_query.startswith(phrase) for phrase in simple_closings_thanks):
            yield "You're welcome! Let me know if there's anything else I can assist with.\n"; return
        if normalized_query in simple_closings_bye:
            yield "Goodbye! Have a great day.\n"; return
        if normalized_query in simple_banter:
            simple_response_messages = [{"role": "system", "content": "You are a friendly assistant. Respond to the user's conversational opening in a brief and engaging way."}, {"role": "user", "content": user_query}]
            async for chunk in llm_service.generate_streamed_response(messages=simple_response_messages, use_smaller_model=True): yield chunk
            yield "\n"; return

        user_profile_summary = self._summarize_user_profile(current_user)
        tool_schemas_for_llm_str = json.dumps(AVAILABLE_TOOLS_SCHEMAS, indent=2)

        current_turn_full_history: List[Dict[str, Any]] = []
        if chat_history:
            current_turn_full_history.extend([msg if isinstance(msg, dict) else msg.model_dump() for msg in chat_history])
        current_turn_full_history.append({"role": "user", "content": f"My current request is: \"{user_query}\""})
        
        executed_tool_calls_this_turn: Set[Tuple[str, frozenset]] = set()
        accumulated_tool_outputs_for_synthesis: List[Dict[str, Any]] = [] 
        contextual_prompt_for_llm_action = f"Based on my original request: \"{user_query}\", and the information gathered so far (if any), what is the next logical step to fully address my request?"

        for iteration in range(MAX_TOOL_ITERATIONS):
            system_prompt_tool_selection = f"""You are a sophisticated and methodical financial assistant. Your primary goal is to accurately understand the user's query, identify ALL necessary pieces of information, and use available tools sequentially to gather them, or decide to answer directly if appropriate.

User Profile:
{user_profile_summary}
Today's Date: {datetime.now().strftime('%Y-%m-%d')}
User ID: {current_user.id} # Provided for context, use it with tools that need user identity
Available Tools (ensure your chosen tool_name and arguments match these schemas exactly):
{tool_schemas_for_llm_str}

**Constraint: Your response for this step MUST be EITHER a single valid JSON tool call OR a plain text final answer/clarification. Do not provide explanations or any other text before or after the JSON if you choose a tool. If you are providing a tool call, your entire response must be ONLY the JSON object.**

**THE ORIGINAL USER REQUEST FOR THIS ENTIRE TURN (Your ultimate goal):**
"{user_query}" 

**ALL INFORMATION GATHERED SO FAR IN THIS ENTIRE TURN (Previous tool calls in this turn and their outputs):**
{json.dumps(accumulated_tool_outputs_for_synthesis, indent=2) if accumulated_tool_outputs_for_synthesis else "No tool calls have been made yet in this turn."}

**Your Iterative Task & Decision Process (Strive for NEW information each step):**
1.  **Analyze Original Request & Progress:**
    * Carefully re-read the "ORIGINAL USER REQUEST".
    * Review "ALL INFORMATION GATHERED SO FAR IN THIS ENTIRE TURN".
    * Consider the overall chat history (provided in earlier messages if this is not the first turn).
    * Identify ALL aspects of the original request that have *not yet* been addressed. What is the *next distinct piece of information* required?
    * For portfolio-related queries (value, PnL, positions): if the user has multiple portfolios, you might first need to use `list_my_portfolios` if the specific portfolio isn't clear from the query. Then, use the relevant portfolio ID or name with other portfolio tools. If only one portfolio exists, tools may default to it if designed that way, or you can infer its ID.

2.  **Tool Selection (Primary Action - Aim for NOVELTY):**
    * If a tool can provide this *new* piece of information, select the single most appropriate tool.
    * **Argument Inference:** If not explicitly provided by the user, infer necessary arguments like stock symbols (e.g., "Apple company" -> "AAPL", "Bitcoin crypto" -> "BTC") or crypto markets. Be precise.
    * **Asset Type Specificity (CRITICAL):** Pay EXTREMELY close attention to tool descriptions. Use `get_stock_price` for stocks (AAPL, MSFT), `get_crypto_price` for crypto (BTC, ETH). If unsure about an asset's type, use `general_web_search` to clarify *before* attempting a price tool.
    * **Complex Queries & Sequential Operations:** Break down the "ORIGINAL USER REQUEST" into sub-questions. Address one sub-question per tool call. Example: "Price of AAPL and BTC" requires two separate `get_stock_price` and `get_crypto_price` calls in two iterations. If a list of items needs processing (e.g., "find 3 pharma stocks and their prices"), first use `general_web_search` to get the list, then use specific tools for each item in subsequent iterations.
    * **Avoiding Redundancy (CRITICAL):** DO NOT re-request information if an identical tool call (same tool_name and arguments) is already listed in "ALL INFORMATION GATHERED SO FAR IN THIS ENTIRE TURN". Choosing a redundant call will result in corrective feedback.
    * **Tool Failure Handling (from previous attempts in THIS turn):** If a tool FAILED previously in *this turn* for specific arguments (e.g., API limit, invalid symbol *for that specific tool*):
        a. If failure was due to an incorrect argument type (e.g., stock symbol for a crypto tool), try the *correct* tool type.
        b. Consider `general_web_search` as a fallback for factual data.
        c. Choose a different, relevant tool if applicable.
        d. If no alternative is clear, you may need to proceed to synthesize an answer acknowledging this gap later.
    * Respond ONLY with a single JSON object for your chosen tool: {{"tool_name": "TOOL_NAME", "arguments": {{"arg1": "value1", ...}}}}

3.  **Direct Answer / Clarification (Alternative Actions):**
    * If ALL parts of the "ORIGINAL USER REQUEST" have been addressed by tool calls in "ALL INFORMATION GATHERED SO FAR...", OR if the request is simple and clearly does not require tools, then respond directly in PLAIN TEXT. Your entire response should be that text, NOT JSON.
    * If the request is ambiguous and you need more information *from the user* to proceed effectively, ask a clarifying question in PLAIN TEXT.

Based on all the above, and the current contextual query/situation described in the latest user message below, decide your next action.
"""
            
            messages_for_llm_decision = []
            messages_for_llm_decision.append({"role": "system", "content": system_prompt_tool_selection})
            messages_for_llm_decision.extend(current_turn_full_history)
            messages_for_llm_decision.append({"role": "user", "content": contextual_prompt_for_llm_action})

            llm_decision_str_raw = await llm_service.generate_response(prompt=None, history=messages_for_llm_decision, is_json=False, use_smaller_model=False)
            current_turn_full_history.append({"role": "assistant", "content": llm_decision_str_raw})
            llm_decision_cleaned_for_parsing = self._clean_llm_json_response(llm_decision_str_raw)

            tool_call_attempted_this_iteration = False
            try:
                tool_call_data = json.loads(llm_decision_cleaned_for_parsing)
                if isinstance(tool_call_data, dict) and "tool_name" in tool_call_data and "arguments" in tool_call_data:
                    tool_name = tool_call_data["tool_name"]
                    tool_args_from_llm = tool_call_data.get("arguments", {})
                    if not isinstance(tool_args_from_llm, dict): tool_args_from_llm = {}

                    tool_call_attempted_this_iteration = True
                    
                    tool_output_str, final_args_used = await self._execute_tool(
                        tool_name, tool_args_from_llm, user_id=current_user.id
                    ) 

                    final_args_for_key_list = []
                    for k, v in sorted(final_args_used.items()):
                        if isinstance(v, (dict, list)):
                            final_args_for_key_list.append((k, json.dumps(v, sort_keys=True)))
                        else:
                            final_args_for_key_list.append((k,v))
                    current_call_signature = (tool_name, frozenset(final_args_for_key_list))

                
                    if current_call_signature in executed_tool_calls_this_turn:
                        feedback_for_llm = f"System Feedback: The tool '{tool_name}' with arguments effectively resulting in {json.dumps(final_args_used)} has ALREADY been successfully processed in this turn. Please choose a tool to gather *different, new* information for the original request ('{user_query}'), or synthesize the final answer if all parts are now covered."
                        current_turn_full_history.append({"role": "user", "content": feedback_for_llm})
                        contextual_prompt_for_llm_action = feedback_for_llm
                        if iteration == MAX_TOOL_ITERATIONS - 1: break
                        else: continue

                    executed_tool_calls_this_turn.add(current_call_signature)
                    accumulated_tool_outputs_for_synthesis.append({ 
                        "tool_name": tool_name, "arguments": final_args_used, "output": tool_output_str
                    })

                    tool_result_feedback_for_history = f"Tool Output from '{tool_name}' (arguments: {json.dumps(final_args_used)}):\n{tool_output_str}"
                    current_turn_full_history.append({"role": "user", "content": tool_result_feedback_for_history}) 
                    
                    contextual_prompt_for_llm_action = f"Okay, the tool '{tool_name}' provided output (see above). Based on my original request: \"{user_query}\", and all information gathered so far, what is the NEXT piece of NEW information needed? Or, if all parts are addressed, provide the final answer in plain text."
                    
                    if iteration == MAX_TOOL_ITERATIONS - 1:
                        break
                else: 
                    if llm_decision_cleaned_for_parsing.strip():
                        async for chunk in self._stream_plain_text(llm_decision_cleaned_for_parsing): yield chunk
                    return

            except json.JSONDecodeError: 
                if llm_decision_cleaned_for_parsing.strip():
                    async for chunk in self._stream_plain_text(llm_decision_cleaned_for_parsing): yield chunk
                return
            if not tool_call_attempted_this_iteration and iteration < MAX_TOOL_ITERATIONS - 1 :
                break
        
        accumulated_outputs_str = "\n\n".join([
            f"Tool: {item['tool_name']}\nArguments: {json.dumps(item['arguments'])}\nOutput:\n{item['output']}"
            for item in accumulated_tool_outputs_for_synthesis
        ])

        system_prompt_synthesis = f"""You are a highly capable, trustworthy, and articulate financial assistant chatbot.
Your primary goal is to provide a single, clear, comprehensive, and helpful answer to the user's original query, based on all information gathered.

User Profile:
{user_profile_summary}
Today's Date: {datetime.now().strftime('%Y-%m-%d')}

The user's original query for this turn was: "{user_query}"

Chat History (from previous turns, for broader context):
{json.dumps(chat_history[-5:], indent=2) if chat_history else "No prior chat history for this session."} 

User messages from THIS CURRENT TURN (including original query and any tool outputs presented as user messages):
{json.dumps([msg for msg in current_turn_full_history if msg['role'] == 'user'], indent=2)}

Information Gathered IN THIS CURRENT TURN using available tools to address the original query:
--- TOOL CALLS AND RESULTS FROM THIS TURN ---
{accumulated_outputs_str if accumulated_tool_outputs_for_synthesis else "No specific information was gathered using tools for this query this turn, or a direct answer was decided earlier."}
--- END OF TOOL CALLS AND RESULTS ---

**Your Task: Synthesize a Final Answer**
Based on ALL the above information (user profile, original query, full chat history context, user messages from this turn, AND all tool outputs from THIS CURRENT TURN), generate a single, natural language response to the user.
-   **Address All Parts of Original Query:** Ensure your answer directly addresses all aspects of "{user_query}".
-   **Natural Tone & Integration:** Speak as if you possess the knowledge directly. Synthesize information from multiple tool calls naturally. **AVOID** phrases like "The tool 'get_stock_quote' returned...", "Based on the web search...", "The arguments for the tool were...". Instead, integrate the information fluidly (e.g., "The current price of Apple (AAPL) is $X and its P/E ratio suggests...").
-   **Handle Errors/Missing Info:** If a tool reported an error, couldn't find specific information, or if no tools were applicable/successful for a part of the query, acknowledge that part gracefully (e.g., "I couldn't find the specific price for XYZ at this moment, but I found..."). Do NOT invent information. If no tools were used, answer based on general knowledge if appropriate, or state inability if it requires data.
-   **Investment Opinions/Advice:** If the query asks for an investment opinion (e.g., "should I buy X?"):
    * Frame it cautiously (e.g., "Some analysts suggest...", "Considering its recent performance...", "Factors to consider include...").
    * ALWAYS include the disclaimer: "This is not financial advice. Always do your own research and consult with a qualified financial professional before making investment decisions."
-   **Clarity & Conciseness:** Provide specific and actionable answers. Briefly explain technical terms if the user profile suggests they are a beginner.
-   **Structure:** Use paragraphs or bullet points for readability if answering multiple points.

Now, generate the comprehensive final response to the user's original query for this turn: "{user_query}"
Your response should be in plain text.
"""
        synthesis_llm_messages = [{"role": "system", "content": system_prompt_synthesis}]
        
        async for chunk in llm_service.generate_streamed_response(
            messages=synthesis_llm_messages,
            use_smaller_model=False
        ):
            yield chunk
        yield "\n"

    async def _stream_plain_text(self, text: str, chunk_size: int = 10) -> AsyncGenerator[str, None]:
        """Streams plain text in small, delayed chunks.

        A helper function to stream a non-LLM string with a similar feel to
        the token-by-token streaming from the LLM service.

        Args:
            text (str): The text to be streamed.
            chunk_size (int): The number of characters per chunk.

        Yields:
            str: The next chunk of the input text.
        """
        for i in range(0, len(text), chunk_size):
            yield text[i:i+chunk_size]
            await asyncio.sleep(0.001)
        if text and not text.endswith("\n"):
            yield "\n"

rag_service = RAGService()